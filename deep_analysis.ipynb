{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d7d6c-b923-4bc0-9c4d-e51d84a90cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from matplotlib import pylab as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f637599-f85a-47ce-9f6c-b657f8202980",
   "metadata": {},
   "source": [
    "#### Merge the 12 months of sales data into a single CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079284fb-5995-49b9-bc16-04d46cabcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a list compreension for all the data in the folder\n",
    "files = [file for file in os.listdir('../data')] \n",
    "# let's make a pandas DataFrame\n",
    "all_months_data = pd.DataFrame()\n",
    "# makes a loop for concat the data\n",
    "for file in files:\n",
    "    data = pd.read_csv('../data/' + file)\n",
    "    all_months_data = pd.concat([all_months_data, data])\n",
    "# export all data to csv    \n",
    "all_months_data.to_csv(\"all_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036150e-0ed7-4e6d-90e1-ba08f4159424",
   "metadata": {},
   "source": [
    "### Read in updated DataFrame\n",
    "###### Let's see the data and how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54188e43-3f4f-4392-bb70-95388d8f7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = pd.read_csv('all_data.csv') # read data\n",
    "sales_data # show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4da29-9702-4899-bf95-6e795250a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Head\"\n",
    "sales_data.head() # Checking the first 5 rows of data\n",
    "# Checking the last 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c638d8e-dce4-4bc6-b499-7dc79d2d477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Tail\"\n",
    "sales_data.tail() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439624f3-3c77-4558-8c43-6ffd1a28672d",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Data preprocessing can refer to manipulation or dropping of data before it is used in order to ensure or\n",
    "enhance performance, and is an important step in the data mining process. The phrase \"garbage in, garbage out\" \n",
    "is particularly applicable to data mining and machine learning projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c5e59-b170-4d17-b336-3e5e82bc06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the information \n",
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ee97a-8581-443c-97d2-a568b9cd9895",
   "metadata": {},
   "source": [
    "### Uniqueness Categorical Variables\n",
    "Let's have a look at categorical variables. How many unique values of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7d11c-0cbb-4b98-8603-b62894d709aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = sales_data.select_dtypes(['category', 'object']).columns # getting the Uniqueness catrgorical variable\n",
    "for col in categorical:\n",
    "    print('{} : {} unique value(s)'.format(col, sales_data[col].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b34f5d-7524-4c37-a02c-04408ca8fe53",
   "metadata": {},
   "source": [
    "### How many missing data points do we have?\n",
    "Ok, now we know that we do have some missing values. Let's see how many we have in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d0d7d-9904-4a81-80c5-64afed801145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing data points per column\n",
    "missing_values_count = sales_data.isnull().sum()\n",
    "# look at the # of missing points in the first ten columns\n",
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7e439-7374-4d0a-8039-f2dfa4c96adf",
   "metadata": {},
   "source": [
    "It might be helpful to see what percentage of the values in our dataset were missing to give us \n",
    "a better sense of the scale of this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d321f9-a07d-4c9a-a8ce-fac5eb7e74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many total missing values do we have?\n",
    "total_cells = np.product(sales_data.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "# percent of data that is missing\n",
    "percent_missing = (total_missing / total_cells) * 100\n",
    "print(f\"{percent_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde424b-b60d-469d-85a6-3a475a2c0238",
   "metadata": {},
   "source": [
    "### Clean up the Data!¶\n",
    "The first step in this is figuring out what we need to clean. I have found in practice, \n",
    "that you find things you need to clean as you perform operations and get errors. \n",
    "Based on the error, you decide how you should go about cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66369ca7-3762-4a39-919d-f283c6188b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the rows of NaN data!\n",
    "sales_data = sales_data.dropna(how='all')\n",
    "# okay, let's check it again!\n",
    "\"NaN Value:\"\n",
    "sales_data[sales_data.isna().any(axis=1)]\n",
    "# future warning! ValueError: invalid literal for int() with base 10: 'Or'\n",
    "\"Clean Future Warnings:\"\n",
    "sales_data = sales_data[sales_data['Order Date'].str[0:2] != 'Or']\n",
    "sales_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f3c2a-983a-4089-9452-02f3ef9e1a3b",
   "metadata": {},
   "source": [
    "### Convert Quantity Ordered column and Price Each column¶\n",
    "Let's convert the Quantity Ordered column and Price Each column to Numeric Type, \n",
    "because we will add some future features, and we \n",
    "need to multiply this two column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebde97b-8b4c-4e21-bb9f-e77e4917c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data\n",
    "sales_data['Quantity Ordered'], sales_data['Price Each'] = sales_data['Quantity Ordered'].astype('int64'), sales_data['Price Each'].astype('float')\n",
    "# and check it \n",
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73325c8-8d0c-4084-b3d0-1670fc818a91",
   "metadata": {},
   "source": [
    "### Convert Order Date column\n",
    "And let's convert Order Date column too, so we can take the Year, Month, and the other date easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0d74c-2c8b-4560-bf46-0c42f6f9deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it using to_datetime() funct\n",
    "sales_data['Order Date'] = pd.to_datetime(sales_data['Order Date'])\n",
    "# let's see it\n",
    "sales_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927dcff-aaa5-42d8-a8b1-f8a1585d915e",
   "metadata": {},
   "source": [
    "### Recap Data\n",
    "This is some point that we have.\n",
    "\n",
    "We have total  186850  records and  6  columns cateogircal type\n",
    "\n",
    "The total of missing value that we have is  0.29167 %\n",
    "\n",
    "Order ID :  178438  unique value(s)\n",
    "\n",
    "Product :  20  unique value(s)\n",
    "\n",
    "Quantity Ordered :  10  unique value(s)\n",
    "\n",
    "Price Each :  24  unique value(s)\n",
    "\n",
    "Order Date :  142396  unique value(s)\n",
    "\n",
    "Purchase Address :  140788  unique value(s)\n",
    "\n",
    "\n",
    "#### Next, we will try to do some exploration and visualization. But we need to do some Data Preparation first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848b708-2d76-417c-bb7d-f97617034bf3",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Data preparation is the act of manipulating raw data into a form that can \n",
    "readily and accurately be analysed, e.g. for business purposes. Data Preparation \n",
    "is a pre-processing step in which data from one or more sources is cleaned and transformed \n",
    "to improve its quality prior to its use in business analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b5535-8263-41cb-b188-71fef00aa9ef",
   "metadata": {},
   "source": [
    "### Add Month, Hour, Minute, Sales, Cities Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc7fa4-48c9-4312-a750-ea787b49c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Adding new features to\n",
    "    our data, adding Month Data,\n",
    "    Hour Data, Minute Data, Sales Data,\n",
    "    and Cities Column\n",
    "    \n",
    "    Returning:\n",
    "        data with new features\n",
    "    \"\"\"\n",
    "    \n",
    "    # funtction to get the city in the data\n",
    "    def get_city(address):\n",
    "        return address.split(',')[1]\n",
    "    \n",
    "    # funtction to get the state in the data\n",
    "    def get_state(address):\n",
    "        return address.split(',')[2].split(' ')[1]\n",
    "\n",
    "    # let's get the year data in order date column\n",
    "    data['Year'] = data['Order Date'].dt.year\n",
    "    \n",
    "    # let's get the month data in order date column\n",
    "    data['Month'] = data['Order Date'].dt.month\n",
    "    \n",
    "    # let's get the houe data in order date column\n",
    "    data['Hour'] = data['Order Date'].dt.hour \n",
    "    \n",
    "    # let's get the minute data in order date column\n",
    "    data['Minute'] = data['Order Date'].dt.minute \n",
    "    \n",
    "    # let's make the sales column by multiplying the quantity ordered colum with price each column\n",
    "    data['Sales'] = data['Quantity Ordered'] * data['Price Each'] \n",
    "    \n",
    "    # let's get the cities data in order date column\n",
    "    data['Cities'] = data['Purchase Address'].apply(lambda x: f\"{get_city(x)} ({get_state(x)})\") \n",
    "    \n",
    "    return data # returning data\n",
    "\n",
    "# and see it\n",
    "sales_data = augment_data(sales_data)\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba351c5-7ede-4c07-9251-0323e2cc62eb",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "Data Analysis is the process of systematically applying statistical and/or \n",
    "logical techniques to describe and illustrate, condense and recap, and evaluate data. \n",
    "Indeed, researchers generally analyze for patterns in observations through the entire\n",
    "data collection phase (Savenye, Robinson,  2004 ).\n",
    "analyze and investigate data sets and summarize their main characteristics, often employing\n",
    "data visualization methods.\n",
    "\n",
    "Or, the easier, you can say in Data Analysis we (Data Scientist or Data Analyst) what ever you \n",
    "want to call that, in this section, we're looking for the correlation and also the relationships \n",
    "between every data (features and labels) or the variables using and applying the statistical \n",
    "and visualization methods for looking some patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77e9f9-6d31-40b5-b883-f4f715cafa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\") # set the seaborn style\n",
    "# let's make a correlation matrix for `cop_data`\n",
    "plt.figure(figsize=(24, 18)) # figure the size\n",
    "sns.heatmap(sales_data.corr(), annot=True) # create a heatmap\n",
    "plt.title(\"Sales Data Correlation\", weight=\"bold\", fontsize=35, pad=30) # title\n",
    "plt.xticks(weight=\"bold\", fontsize=15) # x-ticks\n",
    "plt.yticks(weight=\"bold\", fontsize=15); # y-ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c78933-85bb-4271-9a16-bbd45d2b947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the correlation from `sales_data`\n",
    "(sales_data.corr()['Sales'] # transform it into data corr\n",
    "           .sort_values(ascending=False) # sort values\n",
    "           .to_frame() # change it into data frame\n",
    "           .T) # transpose it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97e679-95b0-4f04-b0d9-d40aa0fa47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical measure of sales data without object type of data\n",
    "sales_data_numeric = sales_data.describe(include=[np.number]) \n",
    "\"Statistical Measure of Sales Data in Numeric Data\"\n",
    "sales_data_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf012ac6-59fb-4dc8-b91b-100f6567429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical measure of sales data without numeric type of data\n",
    "sales_data_object = sales_data.describe(exclude=[np.number],datetime_is_numeric=True)\n",
    "\"Statistical Measure of Sales Data in Object / Str Data\"\n",
    "sales_data_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a11097-42fc-4d88-bce2-232efddc986d",
   "metadata": {},
   "source": [
    "### Univariate Analysis\n",
    "Univariate analysis is perhaps the simplest form of statistical analysis. Like other forms of statistics, \n",
    "it can be inferential or descriptive. The key fact is that only one variable is involved. \n",
    "Univariate analysis can yield misleading results in cases in which multivariate analysis is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6fa46-64ad-40ec-8dc4-7571f04104cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking and visualizing the type of distribution of a feature column\n",
    "def univariate_analysis(data, color, title1, title2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Showing visualization of univariate\n",
    "    analysis with displot and qqplot\n",
    "    visualization from seaborn and statsmodel\n",
    "    library.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame, array, or list of arrays, optional\n",
    "        Dataset for plotting. If ``x`` and ``y`` are absent, this is\n",
    "        interpreted as wide-form. Otherwise it is expected to be long-form. \n",
    "    title1: The title of the visualization, title1 for displot visualization\n",
    "        And title2 for quantile plot from statsmodel.\n",
    "    title2: The title of the visualization, title1 for displot visualization\n",
    "        And title2 for quantile plot from statsmodel.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fig : matplotlib figure\n",
    "        Returns the Figure object with the plot drawn onto it.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots( # subplots\n",
    "        ncols=2, # num of cols\n",
    "        nrows=1, # num of rows\n",
    "        figsize=(20, 6) # set the width and high\n",
    "    )\n",
    "\n",
    "    sns.distplot( # create a distplot visualization\n",
    "        data, # data\n",
    "        ax=ax1, # axes 1\n",
    "        kde=True, # kde\n",
    "        color=color # color\n",
    "    )\n",
    "    \n",
    "    ax1.set_title( # set the title 1\n",
    "        title1, \n",
    "        weight=\"bold\", # weight\n",
    "        fontsize=25, # font-size\n",
    "        pad=30 # padding\n",
    "    ) \n",
    "    \n",
    "    qqplot( # qqplot (quantile plot)\n",
    "        data, # data\n",
    "        ax=ax2, # axes 2\n",
    "        line='s' # line \n",
    "    )\n",
    "    \n",
    "    ax2.set_title( # set the title 2\n",
    "        title2, \n",
    "        weight=\"bold\", # weight\n",
    "        fontsize=25, # font-size\n",
    "        pad=30 # padding\n",
    "    )\n",
    "    \n",
    "    return fig # returning the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e6425-e3cb-4e21-910f-6ccd1b215833",
   "metadata": {},
   "source": [
    "Let's try to find the proportion that lies in between two standard deviation ($\\sigma$) from mean ($\\mu$) using Chebychev's Theorem, and let's try to interprete..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f547842-6eeb-4536-8d85-2650ffb8a595",
   "metadata": {},
   "source": [
    "Chebychev's Theorem\n",
    "\n",
    "$$ \\begin{aligned} 1 - \\frac{1}{k^2}: k &= 2 -> 1 - \\frac{1}{2^2} = \\frac{3}{4} -> 75 \\\\ k &= 3 -> 1 - \\frac{1}{3^2} = \\frac{8}{9} -> 88.9 \\end{aligned} $$\n",
    "How to find Standard Deviation ($\\sigma$)?\n",
    "\n",
    "Here's the Formula:\n",
    "\n",
    "$$ \\begin{aligned} \\sigma &= \\sqrt{\\sigma^2} = \\sqrt{\\frac{\\sum{(x - \\mu)^2}}{N}} \\\\ s &= \\sqrt{s^2} = \\sqrt{\\frac{\\sum{(x - \\bar{x})^2}}{n - 1}} \\end{aligned} $$\n",
    "How to find Mean ($\\mu$)?\n",
    "\n",
    "Here's the Formula:\n",
    "\n",
    "$$ \\begin{aligned} \\mu = \\frac{\\sum{x}}{N} \\\\ \\bar{x} = \\frac{\\sum{x}}{n} \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59b4ac-688a-4506-b88c-96db781a8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity Ordered Data\n",
    "univariate_analysis( # call the function\n",
    "    data=sales_data['Quantity Ordered'], # put the data\n",
    "    color='red', # pick the color\n",
    "    title1='Quantity Ordered Data Distribution', # title1\n",
    "    title2='Quantile Plot' # title2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba4562-79a5-4817-937e-dc95a590d5a8",
   "metadata": {},
   "source": [
    "Here we can see it, the average customer buys $1$ item/product more often, there are also a few customers who buy $2$ or $4$ items/product at once, more than that it is very rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6e726-98a6-48d8-8217-9ca8ce1635b5",
   "metadata": {},
   "source": [
    "Quantity Ordered\n",
    "Find the proportion that lies in between two standard deviation ($\\sigma$) from mean ($\\mu$), and let's try to interprete that. and In the Quantity Ordered Data, the $\\mu = 1.12$ and the $\\sigma = 0.44$, then without further ado let's calculate it.\n",
    "\n",
    "Calculation:\n",
    "$1.12 - 2(0.44) = 0.2$\n",
    "$1.12 + 2(0.44) = 2$\n",
    "Interpretation:\n",
    "At least $75\\%$ of the Sales Data Quantity Ordered population in the USA has a Quantity Ordered range from $0 - 2$ item/product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0571660-5804-4287-ae66-ac6b50489929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Each Data\n",
    "univariate_analysis( # call the function\n",
    "    data=sales_data['Price Each'], # put the data\n",
    "    color='blue', # pick the color \n",
    "    title1='Price Each Data Distribution', # title1 \n",
    "    title2='Quantile Plot' # title2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbeb97-a97c-46ab-a4b9-abb000e14474",
   "metadata": {},
   "source": [
    "### Price Each \n",
    "\n",
    "Find the proportion that lies in between two standard deviation ($\\sigma$) from mean ($\\mu$), and let's try to interprete that. and in the Price Each Data, the $\\mu = 184.3$ and the $\\sigma = 332.7$, then without further ado let's calculate it.\n",
    "\n",
    "Calculation:\n",
    "$184.3 - 2(332.7) = -481$\n",
    "$184.3 + 2(332.7) = 849.7$\n",
    "Interpretation:\n",
    "At least $75\\%$ of the population Sales Price data for each item/product in the USA has a price range for each item/product from $0 - 849.7$ (USD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e71216-b3ee-412e-86a1-645efac85261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales Data\n",
    "univariate_analysis( # call the function\n",
    "    data=sales_data['Sales'], # put the data \n",
    "    color='black', # pick the color\n",
    "    title1='Sales Data Distribution', # title1 \n",
    "    title2='Quantile Plot' # title2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e775a-0b8a-4a29-bbf7-ce611a24108d",
   "metadata": {},
   "source": [
    "### Sales\n",
    "Find the proportion that lies in between two standard deviation ( σ ) from mean ( μ ), and let's try to interprete that. and in the Sales Data, the  μ=185.4  and the  σ=332.9 , then without further ado let's calculate it.\n",
    "\n",
    "Calculation:\n",
    "185.4−2(332.9)=−480 \n",
    "185.4+2(332.9)=851.19 \n",
    "Interpretation:\n",
    "At least  75%  of population Sales Data customers in USA have Sales range from  0−851.19  (USD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b89a44-1450-4802-8522-673d78e8be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking skewness value\n",
    "# if value lies between -0.5 to 0.5  then it is normal otherwise skewed\n",
    "skew_value = sales_data.skew().sort_values(ascending=False).to_frame()\n",
    "skew_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d0732-5ebf-4c2b-a11f-56a929a895ad",
   "metadata": {},
   "source": [
    "It can be seen that most of the data we have are in the form of a normal distribution, and there are two skewed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1a4f3-b6d4-45bb-a9ed-033d5c819e87",
   "metadata": {},
   "source": [
    "### Task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b00f8d-5667-43fe-85e6-55440aaeb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot it\n",
    "plt.figure(figsize=(24, 10)) # figuring the size\n",
    "# makes count plot \n",
    "sns.countplot(\n",
    "    x=\"Year\", \n",
    "    data=sales_data\n",
    ")\n",
    "plt.title( # title\n",
    "    \"What was the best Year for sales? How much was earned that Year?\", \n",
    "    weight=\"bold\", # weiqht\n",
    "    fontsize=35, # font-size\n",
    "    pad=30 # padding\n",
    ")\n",
    "plt.xlabel( # x-label\n",
    "    \"Years\", \n",
    "    weight=\"bold\", # weight\n",
    "    color=\"purple\", # color\n",
    "    fontsize=25, # font-size\n",
    "    loc=\"center\" # location\n",
    ")\n",
    "plt.xticks( # x-ticks\n",
    "    weight=\"bold\", # weight\n",
    "    fontsize=15 # font-size\n",
    ")\n",
    "plt.ylabel( # y-label\n",
    "    \"Sales in USD ($)\", \n",
    "    weight=\"bold\", # weight\n",
    "    color=\"green\", # color\n",
    "    fontsize=20 # font-size\n",
    ")\n",
    "plt.yticks( # y-ticks\n",
    "    weight=\"bold\", # weight \n",
    "    fontsize=15 # font-size\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff0d97-c9db-4675-9aa0-36a6a258aeb3",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "When viewed from the data above,  2019  was the best year that had the highest number of sales,\n",
    "which was  $34,483,365 , compared to  2020  which only had  $8,670  in sales, this is due to the lack of \n",
    "data in  2020  which caused a data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ac1bb-4b36-42fa-8fd8-d42df8b7c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the Month cols\n",
    "sum_of_month_and_earned = sales_data.groupby('Month').sum().astype('int')\n",
    "# let's plot it\n",
    "plt.figure(figsize=(24, 14)) # figuring the size\n",
    "# makes bar plot \n",
    "sns.barplot( # barplot\n",
    "    x=sum_of_month_and_earned.index, # x-axis\n",
    "    y=sum_of_month_and_earned[\"Sales\"], # y-axis\n",
    "    data=sum_of_month_and_earned, # data\n",
    "    palette=\"deep\" # palette\n",
    ")\n",
    "plt.title( # title\n",
    "    \"What was the best month for sales? How much was earned that month?\", \n",
    "    weight=\"bold\", # weight\n",
    "    fontsize=35, # font-size\n",
    "    pad=30 # padding\n",
    ")\n",
    "plt.xlabel( # x-label\n",
    "    \"Months\", \n",
    "    weight=\"bold\", # weight\n",
    "    color=\"purple\", # color\n",
    "    fontsize=25, # font-size\n",
    "    loc=\"center\" # location\n",
    ")\n",
    "plt.xticks( # x-ticks\n",
    "    weight=\"bold\", # weight\n",
    "    fontsize=15 # font-size\n",
    ")\n",
    "plt.ylabel( # y-label\n",
    "    \"Sales in USD ($)\", \n",
    "    weight=\"bold\", # weight\n",
    "    color=\"green\", # color\n",
    "    fontsize=20 # font-size\n",
    ")\n",
    "plt.yticks( # y-ticks\n",
    "    weight=\"bold\", # weight \n",
    "    fontsize=15 # font-size\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded5c3e-30e6-4690-a5b0-5c455c5631c5",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "The best month to sell is shown in the visualization above is December which has a record number of sales reaching  $4,613,443 , sales,This may be because in December there is Christmas, where many people buy groceries to make cakes or toys as gifts for loved ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24013b8-5faa-44e7-9ec3-00fb00a23659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group of the highest number of sales in city\n",
    "highest_number_of_sales = sales_data.groupby('Cities').sum().astype('int')\n",
    "# let's plot it\n",
    "plt.figure(figsize=(24, 14)) # figuring the size\n",
    "# makes bar plot \n",
    "sns.barplot( # barplot\n",
    "    x=highest_number_of_sales.index, # x-axis\n",
    "    y=highest_number_of_sales[\"Sales\"], # y-axis\n",
    "    data=highest_number_of_sales, # data\n",
    "    palette=\"deep\" # palette\n",
    ")\n",
    "plt.title( # title\n",
    "    \"What City had the highest number of Sales?\", \n",
    "    weight=\"bold\", # weight\n",
    "    fontsize=35, # font-size\n",
    "    pad=30 # padding\n",
    ")\n",
    "plt.xlabel( # x-label\n",
    "    \"Cities\", \n",
    "    weight=\"bold\", # weight\n",
    "    color=\"purple\", # color\n",
    "    fontsize=25, # font-size\n",
    "    loc=\"center\" # location\n",
    ")\n",
    "plt.xticks( # x-ticks\n",
    "    weight=\"bold\", # weight\n",
    "    fontsize=15, # font-size\n",
    "    rotation=10\n",
    ")\n",
    "plt.ylabel( # y-label\n",
    "    \"Sales in USD ($)\", \n",
    "    weight=\"bold\", # weight\n",
    "    color=\"green\", # color\n",
    "    fontsize=20 # font-size\n",
    ")\n",
    "plt.yticks( # y-ticks\n",
    "    weight=\"bold\", # weight \n",
    "    fontsize=15 # font-size\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f30c42-6aed-4dfe-bb81-9c710cde143d",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "The city that has the most sales data in the above visualization is San Francisco,\n",
    "with total sales reaching  $8,262,203 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34cdba-b963-4b11-8cbe-3f9e6f27e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prepare the value for the x-axis\n",
    "hours = [hour for hour, df in sales_data.groupby('Hour')]\n",
    "# let's plot it\n",
    "plt.figure(figsize=(24, 10)) # figuring the size\n",
    "# makes bar plot \n",
    "plt.plot( # plot\n",
    "    hours, # x-axis\n",
    "    sales_data.groupby(['Hour']).count() # data\n",
    ")\n",
    "# let's add grid\n",
    "plt.grid(True)\n",
    "plt.title( # title\n",
    "    \"What time should we display adverstisement to maximize likelihood of customer's buying product?\", \n",
    "    weight=\"bold\", # weight\n",
    "    fontsize=35, # font-size\n",
    "    pad=30\n",
    ")\n",
    "plt.xlabel( # x-label\n",
    "    \"Hours\", \n",
    "    weight=\"bold\", # weight\n",
    "    color=\"purple\", # color\n",
    "    fontsize=25, # font-size\n",
    "    loc=\"center\" # location\n",
    ")\n",
    "plt.xticks( # x-ticks\n",
    "    ticks=hours, # labels\n",
    "    weight=\"bold\", # weight\n",
    "    fontsize=15 # font-size\n",
    ")\n",
    "plt.ylabel( # y-label\n",
    "    \"Number of Orders\", \n",
    "    weight=\"bold\", # weight\n",
    "    color=\"black\", # color\n",
    "    fontsize=20 # font-size\n",
    ")\n",
    "plt.yticks( # y-ticks\n",
    "    weight=\"bold\", # weight \n",
    "    fontsize=15 # font-size\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953aaf18-2db7-470e-813a-e85263254fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
